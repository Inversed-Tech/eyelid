//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34097967
// Cuda compilation tools, release 12.4, V12.4.131
// Based on NVVM 7.0.1
//

.version 8.4
.target sm_52
.address_size 64

	// .globl	vec_add_Fd
.const .align 8 .b8 FD_FQ79_MODULUS[16] = {1, 224, 108, 177, 210, 41, 222, 50, 4, 105};
.const .align 8 .u64 FD_FQ79_MODULUS_NEG_INV = -5019920096128344065;

.visible .entry vec_add_Fd(
	.param .u64 vec_add_Fd_param_0,
	.param .u64 vec_add_Fd_param_1,
	.param .u64 vec_add_Fd_param_2,
	.param .u32 vec_add_Fd_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<37>;


	ld.param.u64 	%rd8, [vec_add_Fd_param_0];
	ld.param.u64 	%rd9, [vec_add_Fd_param_1];
	ld.param.u64 	%rd10, [vec_add_Fd_param_2];
	ld.param.u32 	%r2, [vec_add_Fd_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_4;

	cvta.to.global.u64 	%rd29, %rd8;
	mul.wide.u32 	%rd30, %r1, 16;
	add.s64 	%rd31, %rd29, %rd30;
	cvta.to.global.u64 	%rd32, %rd9;
	add.s64 	%rd33, %rd32, %rd30;
	cvta.to.global.u64 	%rd34, %rd10;
	add.s64 	%rd1, %rd34, %rd30;
	ld.global.u64 	%rd12, [%rd31];
	mov.u64 	%rd28, 0;
	ld.global.u64 	%rd13, [%rd33];
	// begin inline asm
	add.cc.u64 %rd11, %rd12, %rd13;
	// end inline asm
	st.global.u64 	[%rd1], %rd11;
	ld.global.u64 	%rd15, [%rd31+8];
	ld.global.u64 	%rd16, [%rd33+8];
	// begin inline asm
	addc.cc.u64 %rd14, %rd15, %rd16;
	// end inline asm
	st.global.u64 	[%rd1+8], %rd14;
	// begin inline asm
	addc.u64 %rd17, %rd28, %rd28;
	// end inline asm
	ld.global.u64 	%rd21, [%rd1];
	ld.const.u64 	%rd22, [FD_FQ79_MODULUS];
	// begin inline asm
	sub.cc.u64 %rd36, %rd21, %rd22;
	// end inline asm
	ld.global.u64 	%rd24, [%rd1+8];
	ld.const.u64 	%rd25, [FD_FQ79_MODULUS+8];
	// begin inline asm
	subc.cc.u64 %rd35, %rd24, %rd25;
	// end inline asm
	// begin inline asm
	subc.u64 %rd26, %rd28, %rd28;
	// end inline asm
	setp.eq.s64 	%p2, %rd26, 0;
	@%p2 bra 	$L__BB0_3;

	ld.global.u64 	%rd36, [%rd1];
	ld.global.u64 	%rd35, [%rd1+8];

$L__BB0_3:
	st.global.u64 	[%rd1], %rd36;
	st.global.u64 	[%rd1+8], %rd35;

$L__BB0_4:
	ret;

}
	// .globl	vec_mul_Fd
.visible .entry vec_mul_Fd(
	.param .u64 vec_mul_Fd_param_0,
	.param .u64 vec_mul_Fd_param_1,
	.param .u64 vec_mul_Fd_param_2,
	.param .u32 vec_mul_Fd_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<76>;


	ld.param.u64 	%rd8, [vec_mul_Fd_param_0];
	ld.param.u64 	%rd9, [vec_mul_Fd_param_1];
	ld.param.u64 	%rd10, [vec_mul_Fd_param_2];
	ld.param.u32 	%r2, [vec_mul_Fd_param_3];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_4;

	cvta.to.global.u64 	%rd20, %rd10;
	mul.wide.u32 	%rd21, %r1, 16;
	add.s64 	%rd1, %rd20, %rd21;
	mov.u64 	%rd19, 0;
	st.global.u64 	[%rd1], %rd19;
	st.global.u64 	[%rd1+8], %rd19;
	cvta.to.global.u64 	%rd22, %rd8;
	add.s64 	%rd23, %rd22, %rd21;
	ld.const.u64 	%rd24, [FD_FQ79_MODULUS_NEG_INV];
	ld.const.u64 	%rd13, [FD_FQ79_MODULUS];
	ld.global.u64 	%rd25, [%rd23];
	cvta.to.global.u64 	%rd26, %rd9;
	add.s64 	%rd27, %rd26, %rd21;
	ld.global.u64 	%rd28, [%rd27];
	mul.lo.s64 	%rd29, %rd28, %rd25;
	st.global.u64 	[%rd1], %rd29;
	mul.lo.s64 	%rd30, %rd24, %rd29;
	mul.lo.s64 	%rd31, %rd30, %rd13;
	mul.hi.u64 	%rd32, %rd30, %rd13;
	add.cc.s64 	%rd33, %rd31, %rd29;
	addc.cc.s64 	%rd34, %rd32, 0;
	mul.hi.u64 	%rd35, %rd28, %rd25;
	ld.global.u64 	%rd36, [%rd23+8];
	ld.global.u64 	%rd37, [%rd27];
	mul.lo.s64 	%rd38, %rd37, %rd36;
	mul.hi.u64 	%rd39, %rd37, %rd36;
	add.cc.s64 	%rd40, %rd35, %rd38;
	addc.cc.s64 	%rd41, %rd39, 0;
	ld.const.u64 	%rd16, [FD_FQ79_MODULUS+8];
	mul.lo.s64 	%rd42, %rd16, %rd30;
	mul.hi.u64 	%rd43, %rd16, %rd30;
	add.cc.s64 	%rd44, %rd42, %rd34;
	addc.cc.s64 	%rd45, %rd43, 0;
	add.cc.s64 	%rd46, %rd44, %rd40;
	addc.cc.s64 	%rd47, %rd45, 0;
	st.global.u64 	[%rd1], %rd46;
	add.s64 	%rd48, %rd47, %rd41;
	st.global.u64 	[%rd1+8], %rd48;
	ld.global.u64 	%rd49, [%rd23];
	ld.global.u64 	%rd50, [%rd27+8];
	mul.lo.s64 	%rd51, %rd50, %rd49;
	mul.hi.u64 	%rd52, %rd50, %rd49;
	add.cc.s64 	%rd53, %rd51, %rd46;
	addc.cc.s64 	%rd54, %rd52, 0;
	st.global.u64 	[%rd1], %rd53;
	mul.lo.s64 	%rd55, %rd24, %rd53;
	mul.lo.s64 	%rd56, %rd55, %rd13;
	mul.hi.u64 	%rd57, %rd55, %rd13;
	add.cc.s64 	%rd58, %rd56, %rd53;
	addc.cc.s64 	%rd59, %rd57, 0;
	ld.global.u64 	%rd60, [%rd23+8];
	ld.global.u64 	%rd61, [%rd27+8];
	mul.lo.s64 	%rd62, %rd61, %rd60;
	mul.hi.u64 	%rd63, %rd61, %rd60;
	add.cc.s64 	%rd64, %rd54, %rd48;
	addc.cc.s64 	%rd65, %rd19, 0;
	add.cc.s64 	%rd66, %rd64, %rd62;
	addc.cc.s64 	%rd67, %rd65, %rd63;
	mul.lo.s64 	%rd68, %rd16, %rd55;
	mul.hi.u64 	%rd69, %rd16, %rd55;
	add.cc.s64 	%rd70, %rd68, %rd59;
	addc.cc.s64 	%rd71, %rd69, 0;
	add.cc.s64 	%rd12, %rd70, %rd66;
	addc.cc.s64 	%rd72, %rd71, 0;
	st.global.u64 	[%rd1], %rd12;
	add.s64 	%rd73, %rd72, %rd67;
	st.global.u64 	[%rd1+8], %rd73;
	// begin inline asm
	sub.cc.u64 %rd75, %rd12, %rd13;
	// end inline asm
	ld.global.u64 	%rd15, [%rd1+8];
	// begin inline asm
	subc.cc.u64 %rd74, %rd15, %rd16;
	// end inline asm
	// begin inline asm
	subc.u64 %rd17, %rd19, %rd19;
	// end inline asm
	setp.eq.s64 	%p2, %rd17, 0;
	@%p2 bra 	$L__BB1_3;

	ld.global.u64 	%rd75, [%rd1];
	ld.global.u64 	%rd74, [%rd1+8];

$L__BB1_3:
	st.global.u64 	[%rd1], %rd75;
	st.global.u64 	[%rd1+8], %rd74;

$L__BB1_4:
	ret;

}
	// .globl	endianness_check
.visible .entry endianness_check(
	.param .u64 endianness_check_param_0,
	.param .u64 endianness_check_param_1,
	.param .u64 endianness_check_param_2,
	.param .u64 endianness_check_param_3
)
{
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd1, [endianness_check_param_0];
	ld.param.u64 	%rd2, [endianness_check_param_1];
	ld.param.u64 	%rd3, [endianness_check_param_2];
	ld.param.u64 	%rd4, [endianness_check_param_3];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd3;
	cvta.to.global.u64 	%rd7, %rd2;
	cvta.to.global.u64 	%rd8, %rd1;
	ld.global.v2.u64 	{%rd9, %rd10}, [%rd8];
	st.global.u64 	[%rd7], %rd9;
	ld.global.v2.u64 	{%rd12, %rd13}, [%rd8];
	mov.u64 	%rd15, 1;
	st.global.u64 	[%rd6], %rd13;
	st.global.u64 	[%rd5], %rd15;
	ret;

}

